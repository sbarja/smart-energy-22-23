{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesión 5. Ejemplo de aprendizaje supervisado: Regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso práctico Mantenimiento Predictivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivo:\n",
    "Desarrollar un modelo que sea capaz de predecir la RUL (Remaining Useful Life), en ciclos, de cada motor según los valores obtenidos mediante sensores.\n",
    "\n",
    "Un segundo objetivo, podría ser clasificar el motor segun si se encuentra en los \"Últimos 15 ciclos\" o no. Esto correspondería a un ejercicio de clasificación, por lo que no se desarrollará durante esta clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importar librerías y datos (FD001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages and view available data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import keras\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "print(os.listdir(\"Data/mantenimiento\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train= pd.read_csv('Data/mantenimiento/train_FD001.txt', sep=\" \", header=None)\n",
    "test = pd.read_csv('Data/mantenimiento/test_FD001.txt', sep=\" \", header=None)\n",
    "print(\"train shape: \", train.shape, \"test shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpieza de datos y EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos NaN\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las dos últimas columnas (son todas NaN)\n",
    "train.drop(train.columns[[26, 27]], axis=1, inplace=True)\n",
    "test.drop(test.columns[[26, 27]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El archivo no contiene nombre de variables. Creamos las etiquetas basándonos en la documentación.\n",
    "index_columns_names =  [\"UnitNumber\",\"Cycle\"]\n",
    "op_settings_columns = [\"Op_Setting_\"+str(i) for i in range(1,4)]\n",
    "sensor_columns =[\"Sensor_\"+str(i) for i in range(1,22)]\n",
    "column_names = index_columns_names + op_settings_columns + sensor_columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name columns\n",
    "train.columns = column_names\n",
    "test.columns = column_names\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Dispone nuestro dataset de variable objetivo?\n",
    "En caso negativo, tendremos que calcularla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Como podemos obtener una columna que indique la Remaining Useful Life (RUL)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta sección calcula la vida útil restante (RUL) en notación T-menos para los datos de entrenamiento\n",
    "# encuentra el último ciclo por número de unidad\n",
    "max_cycle = train.groupby('UnitNumber')['Cycle'].max().reset_index()\n",
    "max_cycle.columns = ['UnitNumber', 'MaxOfCycle']\n",
    "max_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusionar el ciclo max de nuevo en el df original\n",
    "train_merged = train.merge(max_cycle, left_on='UnitNumber', right_on='UnitNumber', how='inner')\n",
    "train_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular el RUL para cada fila\n",
    "Target_RUL = train_merged[\"MaxOfCycle\"] - train_merged[\"Cycle\"]\n",
    "train_merged[\"RUL\"] = Target_RUL\n",
    "\n",
    "# eliminar las columnas innecesarias\n",
    "train_RUL = train_merged.drop(\"MaxOfCycle\", axis=1)\n",
    "train_RUL.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RUL.shape\n",
    "train_RUL.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas:**\n",
    "* ¿Cuantas unidades tenemos?\n",
    "* ¿Cual es el valor máximo de vida útil (RUL) de este dataset?\n",
    "* Si nos dijeran que el motor número 18 es una anomalía, como lo eliminariamos del dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# usamos seaborn para visualizar atributos vs variable objetivo (RUL)\n",
    "explore = sns.PairGrid(data=train_RUL[train_RUL['UnitNumber']<16] ,\n",
    "                 x_vars='RUL',\n",
    "                 y_vars=sensor_columns + op_settings_columns,\n",
    "                 hue=\"UnitNumber\", height=3, aspect=2.5)\n",
    "explore = explore.map(plt.scatter, alpha=0.5)\n",
    "explore = explore.set(xlim=(400,0))\n",
    "explore = explore.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Qué podemos observar en los gráficos anteriores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RUL['Op_Setting_3'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# la configuración operativa 3 es estable, visualicemos las configuraciones operativas 1 y 2 frente a algunos de los sensores más activos\n",
    "g = sns.pairplot(data=train_RUL[train_RUL['UnitNumber']<16],\n",
    "                 x_vars=[\"Op_Setting_1\",\"Op_Setting_2\"],\n",
    "                 y_vars=[\"Sensor_2\", \"Sensor_3\", \"Sensor_4\", \"Sensor_7\", \"Sensor_8\", \"Sensor_9\", \"Sensor_11\", \"Sensor_12\", \"Sensor_13\", \"Sensor_14\", \"Sensor_15\", \"Sensor_17\", \"Sensor_20\", \"Sensor_21\"],\n",
    "                 hue=\"UnitNumber\", aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRÁFICOS QUE NO PUEDEN FALTAR EN UN EDA:\n",
    "* Histograma de los ciclos máximos (para este caso en concreto)\n",
    "* Matriz de correlación\n",
    "* Boxplot/Histograma por característica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of maximum time cycles\n",
    "plt.hist(train_RUL.groupby('UnitNumber')['RUL'].max(),bins=20)\n",
    "plt.xlabel('Max time cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el boxplot para cada uno de los atributos.\n",
    "\n",
    "atributos_boxplot = train_RUL.plot(kind='box', subplots=True, layout=(8, 4), figsize=(20, 15), sharex=False,\n",
    "                                 sharey=False, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los histogramas (distribución de datos) para cada uno de los atributos.\n",
    "\n",
    "histograma = train_RUL.hist(xlabelsize=10, ylabelsize=10, bins=24, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr = train_RUL.corr()\n",
    "\n",
    "plt.figure(figsize=(20,20))  \n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Por qué aparecen columnas/filas blancas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparación de los datos\n",
    "Esto incluye, entre otros:\n",
    "1. Limpieza de datos (en caso de que haya NaN)\n",
    "2. Feature Engineering (eliminar características que son ruido y crear nuevas, si es necesario)\n",
    "3. Escalado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Limpieza de datos**\n",
    "\n",
    "No hay Nan en los datos de entrada y no se eliminarán valores atípicos en este ejemplo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RUL.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Feature Engineering**. \n",
    "- Eliminamos aquellas columnas que no nos den información relevante (son ruido y lo único que pueden hacer es empeorar nuestro modelo). También miraremos qué importancia tiene cada característica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Según el apartado de visualización:**\n",
    "\n",
    "Variables a eliminar: S1, S5, S10, S16, S18 y S19\n",
    "\n",
    "También los settings, ya que hemos visto que no existe ninguna correlación entre ellos y ninguno de los sensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes, pero, vamos a hacer un **ranking de importancia de variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero eliminamos las siguientes columnas: 'UnitNumber', 'Cycle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_RUL.shape)\n",
    "to_drop_1 = ['UnitNumber', 'Cycle'] + op_settings_columns  \n",
    "train_features = train_RUL.drop(to_drop_1, axis = 1)\n",
    "print(train_features.shape)\n",
    "\n",
    "# set up features and target variable \n",
    "y = train_features['RUL']\n",
    "X = train_features.drop(['RUL'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos un random forest simple para determinar algunas de las características más importantes\n",
    "# Se puede utilizar para seleccionar características\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "single_rf = RandomForestRegressor()\n",
    "single_rf.fit(X, y)\n",
    "y_pred = single_rf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de la importancia de las características\n",
    "importances = single_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns    \n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "plt.title(\"Feature ranking\", fontsize = 20)\n",
    "plt.bar(range(X.shape[1]), importances[indices], color=\"b\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices) #feature_names, rotation='vertical')\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.ylabel(\"importance\", fontsize = 18)\n",
    "plt.xlabel(\"index of the feature\", fontsize = 18)\n",
    "plt.show()\n",
    "\n",
    "# list feature importance\n",
    "important_features = pd.Series(data=single_rf.feature_importances_,index=X.columns)\n",
    "important_features.sort_values(ascending=False,inplace=True)\n",
    "print(important_features.head(22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eliminar Variables con poca importancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the graphs as well as random forest feature importance, I will exclude sensors without much valuable information\n",
    "print(train_features.shape)\n",
    "to_drop_2 = [\"Sensor_\"+str(i) for i in [17, 6, 10, 5, 16, 18, 19, 1]]\n",
    "train_final = train_features.drop(to_drop_2, axis = 1)\n",
    "print(train_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Transformación (escalado) de los datos**. \n",
    "- Primero, dividimos los datos atributos y etiquetas.\n",
    "- Después, se escalan los datos (en caso de que se crea necesario).\n",
    "- Finalmente, dividimos los datos en train, test y validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se escalan utilizando el método ``MinMaxScaler()``, que escala y traduce cada atributo individualmente de forma que esté dentro del rango [0, 1]. Esto debe hacerse cuando las escalas de los atributos son diferentes (por ejemplo, Sensor_11 [46.85, 48.53], Sensor_2 [641.21, 644.53] o Sensor_9 [9021.73, 9244.59])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, dividimos los datos en **atributos**: X (características) y **etiquetas**: y (objetivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features X ; Target y \n",
    "X = train_final.drop(['RUL'], axis=1) \n",
    "y = train_final['RUL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Escalamos las características\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X))\n",
    "X_scaled.columns = X.columns\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. División de los datos\n",
    "Los datos se dividen en datos de entrenamiento ``X_train``, ``y_train``, datos de validación ``X_val``, ``y_val`` y datos de prueba ``X_test``, ``y_test``.\n",
    "\n",
    "Funcion ``train_test_split``. Definiremos el porcentaje de los datos de entrada que se utilizaran para validar el modelo. En este ejemplo queremos usar un 55% de los datos de entreno, un 20% de prueba y un 15% de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "test_size = 0.2 # porcentaje de los datos de entrada que utilizaré para validar el modelo\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos modelos:\n",
    "* ``Linear regresión (como benchmark para comparar)``\n",
    "* ``Support Vector Regressor``\n",
    "* ``Random Forest Regressor``\n",
    "* ``Gradient Boosting Regressor``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar modelos\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otros modelos:\n",
    "* `'Ridge'`: Ridge()\n",
    "* `'Lasso'`: Lasso()\n",
    "* `'KNeighborsRegressor'`:  KNeighborsRegressor()\n",
    "* `'ExtraTreeRegressor'`: ExtraTreesRegressor()\n",
    "* `'XGBRegressor'`: XGBRegressor()\n",
    "* `'MLPRegressor'`: MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir folds, metrica de error (ej: balanced_accuracy) y crear modelos\n",
    "num_folds = 5\n",
    "error_metrics = {'neg_root_mean_squared_error', 'r2'}\n",
    "\n",
    "# Lista de modelos a probar\n",
    "models = {('LR', LinearRegression()), \n",
    "          ('SVR', SVR()),\n",
    "          ('RF', RandomForestRegressor()),\n",
    "          ('GBR', GradientBoostingRegressor())\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "\n",
    "# Entreno Cross-validation\n",
    "\n",
    "for scoring in error_metrics:\n",
    "    \n",
    "    results = [] # guarda los resultados de las métricas de evaluación\n",
    "    names = []  # Nombre de cada algoritmo\n",
    "    msg = []  # imprime el resumen del método de cross-validation\n",
    "    \n",
    "    print('Evaluation metric: ', scoring)\n",
    "    \n",
    "    for name, model in models:\n",
    "        print('Model ', name)\n",
    "        cross_validation = KFold(n_splits=num_folds, shuffle=True)\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv=cross_validation, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        resume = (name, cv_results.mean(), cv_results.std())\n",
    "        msg.append(resume)\n",
    "    print(msg)\n",
    "\n",
    "    # Compare results between algorithms\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Compare metric result for algorithms: %s' %scoring)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('Candidate models')\n",
    "    ax.set_ylabel('%s' %scoring)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *7. Ajuste de los hiperparámetros*.\n",
    "\n",
    "Pasos para realizar el ajuste de los hiperparámetros:\n",
    "* Especificar el modelo(s) a ajustar\n",
    "* Especificar una métrica a optimizar\n",
    "* Definir los rangos de los parámetros de búsqueda: *parámetros*\n",
    "* Asignar un método de validación: *KFold*\n",
    "* Buscar los hiperparámetros con los datos de validación: *X_val*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = GradientBoostingRegressor()\n",
    "scoring='r2'\n",
    "params = {\n",
    "    # Number of trees in random forest\n",
    "    'n_estimators': [50, 100, 500],  # default=100\n",
    "    # Maximum features\n",
    "    'max_features': ['sqrt', 5, 10], # default='sqrt'\n",
    "     # Maximum number of levels in tree\n",
    "    'max_depth': [2, 5, None],  #deafult = None\n",
    "}\n",
    "\n",
    "\n",
    "# Search for the best combination of hyperparameters\n",
    "cross_validation = KFold(n_splits=5, shuffle=False)\n",
    "my_cv = cross_validation.split(X_val)\n",
    "gsearch = GridSearchCV(estimator=modelo, param_grid=params, scoring=scoring, cv=my_cv)\n",
    "gsearch.fit(X_val, y_val)\n",
    "\n",
    "# Print best Result\n",
    "print(\"Best result: %f using the following hyperparameters %s\" % (gsearch.best_score_, gsearch.best_params_))\n",
    "means = gsearch.cv_results_['mean_test_score']\n",
    "stds = gsearch.cv_results_['std_test_score']\n",
    "params = gsearch.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluación Final del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate metrics on holdout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(max_depth=2, max_features=5, n_estimators=100)\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "print(\"Gradient Boosting Mean Squared Error: \", mean_squared_error(y_val, y_pred))\n",
    "print(\"Gradient Boosting Mean Absolute Error: \", mean_absolute_error(y_val, y_pred))\n",
    "print(\"Gradient Boosting r-squared: \", r2_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot actual vs predicted Remaining Useful Life for the best model\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_val, y_pred, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Actual RUL')\n",
    "ax.set_ylabel('Predicted RUL')\n",
    "ax.set_title('Remaining Useful Life Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo podríamos mejorar los resultados?**\n",
    "\n",
    "Opción: Trabajar solo con datos que tengan una Actual RUL < 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a comprobar con el dataset de Test\n",
    "También tenemos que importar el archivo de RUL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "y_valid = pd.read_csv('Data/mantenimiento/RUL_FD001.txt', sep=\" \", header=None)\n",
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.drop(1, axis=1, inplace=True)\n",
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = test.groupby('UnitNumber').last().reset_index()\n",
    "X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = X_valid.drop(to_drop_1 + to_drop_2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_s=scaler.fit_transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_valid_s.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(max_depth=2, max_features=5, n_estimators=100)\n",
    "\n",
    "gbr.fit(X_s, y)\n",
    "\n",
    "y_val_pred = gbr.predict(X_valid_s)\n",
    "\n",
    "print(\"Random Forest Mean Squared Error: \", mean_squared_error(y_valid, y_val_pred))\n",
    "print(\"Random Forest Mean Absolute Error: \", mean_absolute_error(y_valid, y_val_pred))\n",
    "print(\"Random Forest r-squared: \", r2_score(y_valid, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot actual vs predicted Remaining Useful Life for the best model (GBM)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_valid, y_val_pred, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Actual RUL')\n",
    "ax.set_ylabel('Predicted RUL')\n",
    "ax.set_title('Remaining Useful Life Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/top_ML.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contenido\n",
    "\n",
    "* Regresión lineal\n",
    "* Regresión lineal con sklearn\n",
    "* Métricas evaluación\n",
    "* Regularización\n",
    "* Visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal\n",
    "\n",
    "**Notación para el modelo de regresión lineal**\n",
    "\n",
    "En un modelo lineal, tendremos un prámetro $\\textbf{y}$ que depende de manera lineal de varios covariantes $\\textbf{x}_i$:\n",
    "\n",
    "$$ \\textbf{y}  =  a_1 \\textbf{x}_1  + \\dots + a_m \\textbf{x}_{m} $$\n",
    "\n",
    "Los términos $a_i$ serán los *parametros* del modelo o *coeficientes*.\n",
    "\n",
    "Si lo escribimos de forma matricial:\n",
    "\n",
    "$$ \\textbf{y}  = X \\textbf{w}$$\n",
    "\n",
    "donde $$ \\textbf{y} = \\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right), \n",
    " X = \\left( \\begin{array}{c} x_{11}  \\dots x_{1m} \\\\ x_{21}  \\dots x_{2m}\\\\ \\vdots \\\\ x_{n1}  \\dots x_{nm} \\end{array} \\right),\n",
    " \\textbf{w} = \\left( \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{array} \\right) $$\n",
    " \n",
    "En un modelo de regresión lineal que solo dependa de una variable, tendremos:\n",
    "\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$\n",
    "\n",
    "Con un parámetro $a_0$ llamado constante o corte con el eje de ordenadas.\n",
    "\n",
    "Si tenemos una regresión multivariable, tendremos:\n",
    "$$ \\textbf{y} = a_1 \\textbf{x}_1 + \\dots + a_m \\textbf{x}_m = X \\textbf{w} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n",
    "%matplotlib inline \n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si creamos un conjunto de datos \"aleatorios\", y los graficamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X1 = np.random.randn(300, 2)  # Aleatorios siguiendo una Gauss\n",
    "A = np.array([[0.6, .4], [.4, 0.6]]) # Aplicación lineal para hacerla \"una función lineal\"\n",
    "X2 = np.dot(X1, A)\n",
    "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está claro que hay una cierta correlación entre ellos que podríamos ver con este modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=[0+1*x for x in np.arange(-2,3)] # un posible modelo\n",
    "\n",
    "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3);\n",
    "plt.plot(np.arange(-2,3), model,'r'); \n",
    "for xi, yi in zip(X2[:, 0],X2[:, 1]):\n",
    "    plt.plot([xi]*2, [yi, 0+1*xi], \"k:\") # ilustrar errores\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero, ¿Cuál es el modelo que mejor se ajusta a estos datos? \n",
    "\n",
    "¿Cómo encontramos los parámetros o coeficientes de la siguiente ecuación?\n",
    "\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3);\n",
    "#más modelos!!\n",
    "model1=[0+1*x for x in np.arange(-2,3)]\n",
    "model2=[0.3+0.8*x for x in np.arange(-2,3)]\n",
    "model3=[0-1.2*x for x in np.arange(-2,3)]\n",
    "plt.plot(np.arange(-2,3), model1,'r')\n",
    "plt.plot(np.arange(-2,3), model2,'g')\n",
    "plt.plot(np.arange(-2,3), model3,'y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo siempre será minimizar la suma del cuadrado de la distancia entro los puntos reales y el valor de la funcióm\n",
    "\n",
    "Si tenemos los datos $(\\textbf{x},\\textbf{y})$, queremos minimizar:\n",
    "\n",
    "$$ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,$$ \n",
    "\n",
    "Esta expresión, se conoce como **sum of squared errors of prediction (SSE)**.\n",
    "\n",
    "La manera más fàcil de encontrar estos dos parámetros es usando el algoritmo OLS (*Ordinary Least Squares*)\n",
    "\n",
    "$$\\textbf{y} = a_0+a_1 \\textbf{x}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  OLS\n",
    "\n",
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0])\n",
    "x_train = x.reshape(1,-1) \n",
    "\n",
    "y = np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])\n",
    "y_train = y.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizar las distancias\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "sse = lambda b, x, y: np.sum((y - b[0] - b[1]*x) ** 2) # Guardar la SSE\n",
    "\n",
    "b0,b1 = fmin(sse, [0,1], args=(x,y)); # Minimizar usando 0 y 1 como valores iniciales de los coeficientes\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10], alpha=0.8) # linia regresión\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, b0+b1*xi], \"k:\") # ilusrtrar errores\n",
    "plt.xlim(2, 9); plt.ylim(0, 20)\n",
    "\n",
    "print('\\nSSE = ', np.sum((y - b0 - b1*x) ** 2))\n",
    "print('\\nModelo obtenido:\\n',round(b0,2),'+',round(b1,2),'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podríamos minimizar otros valores como  **suma del valor absoluto de las diferencias**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizar valores absolutos\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "sabs = lambda b, x, y: np.sum(np.abs(y - b[0] - b[1]*x)) \n",
    "\n",
    "b0,b1 = fmin(sabs, [0,1], args=(x,y)) # Minimizar valores absolutos ahora\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10]) # linia regresión\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, b0+b1*xi], \"k:\") # ilusrtrar errores\n",
    "plt.xlim(2, 9); plt.ylim(0, 20) \n",
    "\n",
    "print('\\nSSE = ', np.sum((y - b0 - b1*x) ** 2))\n",
    "print('Valores Absolutos = ', np.sum(np.abs(y - b0 - b1*x)))\n",
    "print('\\nModelo obtenido:\\n',round(b0,2),'+',round(b1,2),'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, penalizamos menos los valores lejanos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ventajas OLS\n",
    "\n",
    "+ Fácil de calcular computacionalmente cuando son datasets pequeños. Para conjuntos más grandes el cálculo de una inversa provoca un aumento del tiempo de cómputo.\n",
    "+ Fácil de interpretar\n",
    "\n",
    "Y el modelo obtenido es:\n",
    "\n",
    "$$\\widehat{\\textbf{y}} = \\widehat{a}_0+\\widehat{a}_1 \\textbf{x}$$\n",
    "\n",
    "Los sombreros indican que són valores estimados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Otra manera de calcular los coeficientes es el método conocido como Gradient Descent que también se ha visto en otros algoritmos de machine learning.\n",
    "\n",
    "Este proceso consta de los siguientes pasos:\n",
    "* Calcular el error (MSE) del modelo con un parámetro inicial:\n",
    "$$MSE(a_{1})=\\frac{1}{n} \\sum_{i=1}^n (\\widehat{y}^i-y^i)^2,$$ \n",
    "* Calcular la derivada del error en aquel punto.\n",
    "$$\\frac{d}{da_{1}}MSE(a_{1}) $$\n",
    "* Actualizar el parámetro moviendolo $ \\alpha $ veces la dirección de la derivada (*learning rate*)\n",
    "$$a_{1} = a_{1} - \\alpha \\frac{d}{da_{1}}MSE(a_{1})$$\n",
    "\n",
    "El objetivo es buscar el mínimo del error. \n",
    "\n",
    "![ChessUrl](https://s3.amazonaws.com/dq-content/237/single_var_operation.gif \"Gradient\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ejemplo se podría implementar con este código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada(a1, xi_list, yi_list):\n",
    "    len_data=len(xi_list)\n",
    "    deriv=0\n",
    "    for value in range(len_data):\n",
    "        it=xi_list[value]*(a1*xi_list[value]-yi_list[value])\n",
    "        deriv+=it\n",
    "    return (2/len_data)*deriv\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):\n",
    "    a1_list = [a1_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        deriv = derivada(a1, xi_list, yi_list)\n",
    "        a1_new = a1 - alpha*deriv\n",
    "        a1_list.append(a1_new)\n",
    "    return(a1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_iterations = gradient_descent(x, y, 200, 0.3, 2)\n",
    "final_param = param_iterations[-1]\n",
    "print('\\nModelo obtenido:\\n','0','+',round(final_param,2),'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se trabaja con más variables, el metodo es parecido:\n",
    "\n",
    "![ChessUrl](https://s3.amazonaws.com/dq-content/237/surface_plot.gif \"Gradient mas parametros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a1_derivative(a0, a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += xi_list[i]*(a0 + a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def a0_derivative(a0, a1, xi_list, yi_list):\n",
    "    len_data = len(xi_list)\n",
    "    error = 0\n",
    "    for i in range(0, len_data):\n",
    "        error += (a0 + a1*xi_list[i] - yi_list[i])\n",
    "    deriv = 2*error/len_data\n",
    "    return deriv\n",
    "\n",
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):\n",
    "    a1_list = [a1_initial]\n",
    "    a0_list = [a0_initial]\n",
    "\n",
    "    for i in range(0, max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        a0 = a0_list[i]\n",
    "        \n",
    "        a1_deriv = a1_derivative(a0, a1, xi_list, yi_list)\n",
    "        a0_deriv = a0_derivative(a0, a1, xi_list, yi_list)\n",
    "        \n",
    "        a1_new = a1 - alpha*a1_deriv\n",
    "        a0_new = a0 - alpha*a0_deriv\n",
    "        \n",
    "        a1_list.append(a1_new)\n",
    "        a0_list.append(a0_new)\n",
    "    return(a0_list, a1_list)\n",
    "\n",
    "a0_params, a1_params = gradient_descent(x, y, 200, .3, 2, 0)\n",
    "print('\\nModelo obtenido:\\n',round(a0_params[-1],2),'+',round(a1_params[-1],2),'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión lineal con Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Cargar dataset ejemplo\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para ejecutar el modelo, es así de fácil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Crear modelo regresion lineal\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Entrenar\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Obtener modelo\n",
    "print('a1 es: \\n', regr.coef_)\n",
    "print('a0 es: \\n', regr.intercept_)\n",
    "\n",
    "# Plot \n",
    "plt.scatter(diabetes_X_train, diabetes_y_train,  color='black')\n",
    "plt.plot([-0.1,0.2], [b0, regr.intercept_+regr.coef_[0]*0.1]) # linia regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une vez obtenido el modelo también podemos hacer predicciones directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicciones\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# Plot \n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de evaluación\n",
    "\n",
    "Se puede evaluar el modelo obtenido calculando el **mean squared error** ($MSE$) y el **coeficiente de determinación** $R^2$\n",
    "\n",
    "El MSE, se calcula como:\n",
    "\n",
    "$$MSE=\\frac{1}{n} \\sum_{i=1}^n (\\widehat{y}^i-y^i)^2,$$ \n",
    "\n",
    "El coeficiente $R^2$ se define \n",
    "$$(1 - \\textbf{u}/\\textbf{v})$$, donde $\\textbf{u}$ es la suma de los cuadrados de los errores: $$\\textbf{u}=\\sum (\\textbf{y} - \\widehat{\\textbf{y}} )^2$$ \n",
    "donde ${\\textbf{y}}$ son los valores observados y  $\\widehat{\\textbf{y}}$ los valores de la predicción.\n",
    "\n",
    "Y $\\textbf{v}$ es el total de la suma de los cuadrado: $$\\textbf{v}=\\sum (\\textbf{y} - \\bar{\\textbf{y}})^2,$$ donde $\\bar{\\textbf{y}}$ es la media de los datos observados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Sklearn, podríamos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coeficiente de determinación: %.2f'\n",
    "      % r2_score(diabetes_y_test, diabetes_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización (opcional)\n",
    "\n",
    "La regularización penaliza aquellos coeficientes que son muy grandes, reduciendo la complejidad del modelo y puede ser de gran ayuda cuando se tengan problemas de *overfitting*\n",
    "\n",
    "Para ello, al término a minimizar se añade un termino más. Así, a la suma de los cuadrados de los errores (SSE), ahora el objetivo es minimizar:\n",
    "\n",
    "$$ \\textrm{$argmin$}_{\\textbf{w}} \\left( \\frac{1}{2n}  || X \\textbf{w} -  \\textbf{y} ||^2_2 + \\alpha || \\textbf{w}||_p \\right)$$\n",
    " \n",
    "donde $||\\textbf{w}||_p$ es la $\\ell_p$-norma del vector de parámetros.  p = 2 (Ridge) y p = 1 es Lasso.\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"Figures/regularization_ridge_lasso.png\"></center>\n",
    "\n",
    "https://www.youtube.com/watch?v=Xm2C_gTAl8c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización\n",
    "\n",
    "Podemos usar la función ``lmplot()`` de Seaborn para visualizar relaciones lineales de datasets multidimensionales. El input debe ser en *Pandas* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1:  Macroeconomic dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macroeconomic data from 1947 to 1962.\n",
    "\n",
    "Queremos predecir  ('Employed') como respuesta $\\textbf{y}$ usando ('GNP') como predictor $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"GNP\", \"Employed\", df, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos\n",
    "+ los puntos observados en scatterplot\n",
    "+ La línea de regresión obtenida con un intervalo de confianza de un 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"GNP\", \"Population\", df, aspect=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"Armed.Forces\", \"Unemployed\", df, aspect=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay partes poco \"lineales\".\n",
    "\n",
    "Pra ello podemos usar la regresión polinomial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression and Polynomial Regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque se llame regresión *lineal*, también podemos adaptar funciones no lineales. La regresión será lineal en sus parámetros no necesariamente en sus predictores. Si se añaden transformaciones no lineales al modelo de regresión lineal, el modelo puede pasar a ser no lineal\n",
    "\n",
    "$$ \\textbf{y} = a_1 \\phi(\\textbf{x}_1) + \\dots + a_m \\phi(\\textbf{x}_m) $$\n",
    "\n",
    "Esta técnica se conoce como *Polynomial Regression*, donde cuando más alto sea el grado del polinomio que se aplique más complejo puede ser el modelo (vigilar con overfitting!! i tiempo de cómputo!!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, un modelo cúbico:\n",
    "\n",
    "$$y_i \\approx a_0 + a_1 x_i + a_2 x_i^2 + a_3 x_i^3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the order to estimate a polynomial regression\n",
    "sns.lmplot(\"Armed.Forces\", \"Unemployed\", df, order=2, aspect=2);\n",
    "sns.lmplot(\"Armed.Forces\", \"Unemployed\", df, order=3, aspect=2);\n",
    "sns.lmplot(\"Armed.Forces\", \"Unemployed\", df, order=4, aspect=2);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
